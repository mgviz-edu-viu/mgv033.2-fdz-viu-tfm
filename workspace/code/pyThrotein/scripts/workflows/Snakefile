# Import the Snakemake library
import snakemake

# Import the datetime module
from datetime import datetime

# Define a global variable for the current date in ISO format
iso_date = datetime.now().isoformat()


# Load the configuration file
configfile: "config.yaml"

# Define variables for paths and directories
working_directory: config.get("working_directory", "/path/to/working_directory")
data_dir: config.get("data_dir", "/path/to/data_directory")
software_base_dir: config.get("software_base_dir", "/path/to/software_directory")
temp_dir: config.get("temp_dir", "/path/to/temporary_directory")
workflow_dir: config.get("workflow_dir", "/path/to/workflow_directory")


# Define the Excel file path from the configuration
# excel_file: config["excel_file"]
# metadata_json: f"{config['data_dir']}/processed_data/metadata.json"


rule all:
    input:
        f"{config['data_dir']}/processed_data/test_db.OK",
        #config["db_file"],


# Rule 1: Read Excel Metadata and Create JSON
rule extract_metadata:
    input:
        config["excel_file"]
    output:
        config['metadata_json']
    shell:
        """
        # Command to read Excel and create JSON
        python extract_pyThrotein_metadata.py -i {input} -o {output}
        """

# Rule 2: Create DB Model from JSON Metadata
rule create_db_model:
    input:
        config['metadata_json']
    output:
        model=config["db_model_sql"],
        sql_script=config["create_db_sh"]
    shell:
        """
        # Command to create DB model from JSON metadata
        python create_db_model.py --input {input} --output-sql {output.model} \
               --output-bash {output.sql_script} --db-path {config["db_file"]}
        """

rule initialize_db:
    input:
        config["db_model_sql"]
    output:
        config["db_file"]
    shell:
        f"bash {config['create_db_sh']}"

# Rule 4: Read Excel Sheet with Purification Data
rule read_purification_excel:
    input:
        config['excel_file']
    output:
        # Output CSV files for each purification method
        expand(f"{config['data_dir']}/processed_data/purification_method_{{method}}.csv", method=config['purification_methods'])
    script:
        "split_purification_excel.py --input {input}"

# Rule 5: Load Purification Data into the Database
    rule load_data_into_db:
        input:
            method_file= f"{config['data_dir']}/processed_data/purification_method_{wildcards.method}.csv"
    output:
        log_file=lambda wildcards: f"{config['data_dir']}/processed_data/loaded_data_for_{wildcards.method}-{iso_date}.log"
    shell:
        """
        python load_data_into_db.py --input {input.method_file} --output {output.log_file}
        """

# Rule 6: Check Database
rule check_db:
    input:
        log_file=lambda wildcards: f"{config['data_dir']}/processed_data/loaded_data_for_{wildcards.method}-{iso_date}.log"
    output:
        touch(f"{config['data_dir']}/processed_data/test_db.OK")
    shell:
        """
        # Add your check logic here
        echo "Checking the database..."
        # Add any necessary commands to check the database
        """

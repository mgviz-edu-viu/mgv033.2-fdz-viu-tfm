"""
Pipeline for processing the Pythrotein
=======================================

dry run: $ snakemake -np
generate dag svg: $ snakemake -np --dag | dot -Tsvg > dag-full.svg

"""
# Import the Snakemake library
import snakemake

# Import the datetime module
from datetime import datetime

# Define a global variable for the current date in ISO format
iso_date = datetime.now().strftime('%Y-%m-%dT%H%M%S')


# Load the configuration file
configfile: "config.yaml"

# Define variables for paths and directories
working_directory: config.get("working_directory", "/path/to/working_directory")
data_dir: config.get("data_dir", "/path/to/data_directory")
software_base_dir: config.get("software_base_dir", "/path/to/software_dire}ctory")
temp_dir: config.get("temp_dir", "/path/to/temporary_directory")
workflow_dir: config.get("workflow_dir", "/path/to/workflow_directory")

# LOG_FILES=expand(f"{config['data_dir']}/processed_data/loaded_data_for_{{method}}-{iso_date}.log", method=config['purification_methods'])

# Define the Excel file path from the configuration
# excel_file: config["excel_file"]
# metadata_json: f"{config['data_dir']}/processed_data/metadata.json"

rule all:
    input:
        #expand(f"{config['data_dir']}/processed_data/log/test_db-{{method}}.OK", method=config["purification_methods"]).
        f"{config['data_dir']}/processed_data/experiments_list-{iso_date}.txt",
        f"{config['report_dir']}/logs/rule8-experiment_pdf_reports.DONE"
        #config["db_file"]


# Rule 1: Read Excel Metadata and Create JSON
rule r1_extract_metadata:
    input:
        config["excel_file"]
    output:
        config['metadata_json']
    shell:
        """
        # Command to read Excel and create JSON
        python extract_pyThrotein_metadata.py -i {input} -o {output}
        """

# Rule 2: Create DB Model from JSON Metadata
rule r2_create_db_model:
    input:
        config['metadata_json']
    output:
        model=config["db_model_sql"],
        sql_script=config["init_db_sh"]
    params:
        db_path=config["db_file"]
    shell:
        """
        # Command to create DB model from JSON metadata
        python create_db_model.py --input {input} --output-sql {output.model} \
               --output-bash {output.sql_script} --db-path {params.db_path}
        """

#rule 3
rule r3_initialize_db:
    input:
        config["db_model_sql"]
    output:
        #config["db_file"],
        touch(f"{config['data_dir']}/processed_data/logs/db-initialization.DONE")
    shell:
        f"bash {config['init_db_sh']}"

# Rule 4: Read Excel Sheet with Purification Data
rule r4_read_and_split_purification_excel:
    input:
        excel=config['excel_file'],
        status=f"{config['data_dir']}/processed_data/logs/db-initialization.DONE"
    output:
        # Output CSV files for each purification method
        expand(f"{config['data_dir']}/processed_data/purification_method_{{method}}.csv", method=config['purification_methods'])
    params:
        methods=",".join(config["purification_methods"])
    script:
        """
            echo "{params.methods}"
            split_purification_excel.py --input {input.excel} --outdir f"{config['data_dir']}/processed_data/" \
                                        --methods {params.methods})
        """

# Rule 5: Load Purification Data into the Database
rule r5_load_data_into_db:
    input:
        method_file=f"{config['data_dir']}/processed_data/purification_method_{{method}}.csv"
    output:
        status=f"{config['data_dir']}/processed_data/logs/status_db_load-{{method}}.DONE"
    shell:
        """
        python load_data_into_db.py --input {input.method_file};
        touch {output.status}
        """

# Rule 6: Check Database
rule r6_check_db:
    """
    - Check that ALL output status DONE are created, 
    - then check the overall db integrity 
    - and create the status-ok for the database  
    """
    input:
        status_files=expand(f"{config['data_dir']}/processed_data/logs/status_db_load-{{method}}.DONE", method=config['purification_methods'])
    output:
        check_result_ok=f"{config['data_dir']}/processed_data/logs/test_db.OK"
    params:
        methods=",".join(config['purification_methods']),
        data_dir=config['data_dir']
    shell:
        """
        # Add your check logic here
        echo "Checking the database..."
        # Add any necessary commands to check the database
        check_db.py --tables "{params.methods}" --output_tt "{params.data_dir}/processed_data/logs/test_db.{{RESULT}}"
        """
# rule 7 gathered_analysis_report
rule r7_1_gathered_analysis_report:
    input:
        db_status=f"{config['data_dir']}/processed_data/logs/test_db.OK"
    output:
        pdf_report=f"{config['report_dir']}/report-{iso_date}.pdf"
    params:
        db_file=config["db_file"]
    shell:
        """
        python create_pythrotein_general_report.py --input {input.db_status} --output {output.pdf_report} --db {params.db_file}
        """

# rule 8 reports pdf per experiment
#    """
#    When database is done, ask for each experiment and its related purifications and create a pdf for each experiment.
#    """
rule r8_1_get_experiment_list:
    input:
        db_status=f"{config['data_dir']}/processed_data/logs/test_db.OK"
    output:
        experiments_list=f"{config['data_dir']}/processed_data/experiments_list-{iso_date}.txt" 
    shell:
        """
        get_experiment_list.py --input {input.db_status} --output {output.experiments_list}
        """

rule r8_2_generate_report_by_experiment:
    input: 
        experiments_list=f"{config['data_dir']}/processed_data/experiments_list-{iso_date}.txt"
    output:
        touch(f"{config['report_dir']}/logs/rule8-experiment_pdf_reports.DONE")
    params:
        pdf_reports_dir=config.get('pdf_individual_reports_dir', f"{config['report_dir']}/reports_by_experiment/exp_rep-{iso_date}")
    shell:
        """
        mkdir {params.pdf_reports_dir}
        python create_pythrotein_report.py --outdir {params.pdf_reports_dir}
        """

